{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiscale Programming Tutorial\n",
    "*By R. Bulanadi, 28/01/20*\n",
    "\n",
    "***\n",
    "\n",
    "This is the tutorial on actually programming more complicated features into the Multiscale project. Most of this is briefly covered in comments in the core module, but explained in more detail here.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cheat Sheet\n",
    "\n",
    "**How do I access attributes?**\n",
    "\n",
    "To access attributes in a function, there are two steps:\n",
    "1. Pass the name of the attribute into m_apply under the argument `use_attrs`. eg, `use_attrs = 'catch_rate'` would call the attribute `catch_rate` from the source\n",
    "2. Make your custom function have an kwarg named `source_###`, where `###` is the attribute called above, ie, the function would need an argument `source_catch_rate`.\n",
    "\n",
    "**How do I write attributes?**\n",
    "\n",
    "* Some generic attributes are written by default by `write_generic_attributes`, such as `path`, `time`, and `operation_number`. As this is called by `m_apply` and the defunct `write_output_f`, these should always be written.\n",
    "* Additional kwargs passed into `m_apply` will always be written as an attribute by `m_apply`\n",
    "* By extension, as source attributes become additional kwargs, source attributes will also be written as a new attribute by `m_apply`\n",
    "* Attributes can be copied and propagated by using the argument `prop_attrs` in `m_apply`.\n",
    "* If you want an intermediate result or side effect to be saved as an attribute, then use `hdf5_dict` in the custom function, and place write those attributes as kwargs to `hdf5_dict`. Return the result.\n",
    "\n",
    "**How are an unusual inputs/outputs dealt with?**\n",
    "\n",
    "* *Multiple inputs per output:* `l_apply` can be used if more than one input is needed.\n",
    "* *Multiple outputs per input:* In `m_apply`, pass a list of names into `output_names`\n",
    "* *Multiple inputs, and multiple outputs per input:* Pass a list of `output_names` into `l_apply`\n",
    "* *Dimension changes:* By default, `m_apply` should be able to transform m-dimensional data into n-dimensional data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_apply(filename, function, in_paths, output_names=None, folder_names = None,\n",
    "            use_attrs = None, prop_attrs = None, increment_proc = True, **kwargs):\n",
    "    \n",
    "    #Convert in_paths to a list if not already\n",
    "    if type(in_paths) != list:\n",
    "        in_paths = [in_paths]\n",
    "    \n",
    "    #Guess output_names (aka channel names) if not given\n",
    "    if output_names is None:\n",
    "        output_names = in_paths[0].rsplit('/', 1)[1]\n",
    "    \n",
    "    #Guess folder_names (aka sample names) if not given\n",
    "    if folder_names is None:\n",
    "        folder_names = in_paths[0].rsplit('/', 2)[1]\n",
    "    \n",
    "    #Convert output_names to list if not already\n",
    "    if type(output_names) != list:\n",
    "        output_names = [output_names]\n",
    "    \n",
    "    #Convert prop_attrs to list if it exists, but not already a list\n",
    "    if prop_attrs is not None:\n",
    "        if type(prop_attrs) != list:\n",
    "            prop_attrs = [prop_attrs]\n",
    "            \n",
    "    #Convert use_attrs to list if it exists, but not already a list\n",
    "    if use_attrs is not None:\n",
    "        if type(use_attrs) != list:\n",
    "            use_attrs = [use_attrs]\n",
    "    \n",
    "    #Convert file to hdf5 if not already\n",
    "    if filename.split('.')[-1] != 'hdf5':\n",
    "        if os.path.isfile(filename.split('.')[0] + '.hdf5'):\n",
    "            filename = filename.split('.')[0] + '.hdf5'\n",
    "        else:\n",
    "            try:\n",
    "                read_file.tohdf5(filename)\n",
    "                filename = filename.split('.')[0] + '.hdf5'\n",
    "                print('The file does not have an hdf5 extension. It has been converted.')\n",
    "            except:\n",
    "                print('The given filename does not have an hdf5 extension, and it was not possible' \\\n",
    "                        'to convert it. Please use an hdf5 file with m_apply')\n",
    "                \n",
    "    #Open hdf5 file to extract data, attributes, and run function\n",
    "    data_list = []\n",
    "    prop_attr_keys = []\n",
    "    prop_attr_vals = []\n",
    "    use_attr_keys = []\n",
    "    use_attr_vals = []\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        for path in in_paths:\n",
    "            data_list.append(np.array(f[path]))\n",
    "            if prop_attrs is not None:\n",
    "                for prop_attr in prop_attrs:\n",
    "                    if (prop_attr not in prop_attr_keys) and (prop_attr in f[path].attrs):\n",
    "                        prop_attr_keys.append(prop_attr)\n",
    "                        prop_attr_vals.append(f[path].attrs[prop_attr])\n",
    "            if use_attrs is not None:\n",
    "                for use_attr in use_attrs:\n",
    "                    if (use_attr not in use_attr_keys) and (use_attr in f[path].attrs):\n",
    "                        use_attr_keys.append(use_attr)\n",
    "                        use_attr_vals.append(f[path].attrs[use_attr])\n",
    "                for key_num in range(len(use_attr_keys)):\n",
    "                    use_attr_dict = {'source_'+use_attr_keys[key_num]:use_attr_vals[key_num]}\n",
    "                kwargs.update(use_attr_dict)\n",
    "        result = function(*data_list, **kwargs)\n",
    "    \n",
    "    #End function if no result is calculated\n",
    "    if isinstance(result, type(None)):  # type(result) == type(None):\n",
    "        return None\n",
    "\n",
    "    #Convert result to tuple if not already\n",
    "    if type(result) != tuple:\n",
    "        result = tuple([result])\n",
    "    \n",
    "    #Open hdf5 file to write new data, attributes\n",
    "    with h5py.File(filename, 'a') as f:\n",
    "        num_proc = len(f['process'].keys())\n",
    "        if increment_proc:\n",
    "            num_proc = num_proc + 1\n",
    "        out_folder_location = ('process/' + str(num_proc).zfill(3) + '-' + function.__name__ + '/'\n",
    "                               + folder_names)\n",
    "        fproc = f.require_group(out_folder_location)\n",
    "        \n",
    "        if (len(output_names) == len(result)):\n",
    "            for i in range(len(output_names)):\n",
    "                name = output_names[i]\n",
    "                data = result[i]\n",
    "                if type(data)==dict:\n",
    "                    if 'hdf5_dict' in data:\n",
    "                        dataset = create_dataset_from_dict(f[out_folder_location], name, data)\n",
    "                        if prop_attrs is not None:\n",
    "                            dataset = propagate_attrs(dataset, prop_attr_keys, prop_attr_vals)\n",
    "                    else:\n",
    "                        dataset = f[out_folder_location].create_dataset(name, data=data)\n",
    "                        if prop_attrs is not None:\n",
    "                            dataset = propagate_attrs(dataset, prop_attr_keys, prop_attr_vals)\n",
    "                else:\n",
    "                    dataset = f[out_folder_location].create_dataset(name, data=data)\n",
    "                    if prop_attrs is not None:\n",
    "                        dataset = propagate_attrs(dataset, prop_attr_keys, prop_attr_vals)\n",
    "                write_generic_attributes(fproc[name], out_folder_location+'/', in_paths, name)\n",
    "        else:\n",
    "            print('Error: Unequal amount of outputs and output names')\n",
    "        for key, value in kwargs.items():\n",
    "            dataset.attrs[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`m_apply` requires three arguments, natively has five optional arguments, and passes all additional keyword arguments to the declared function. The required arguments are:\n",
    "\n",
    "1. `filename`: The filename of the `.hdf5` file operated on\n",
    "2. `function`: The function applied to the file\n",
    "3. `in_paths`: An explicit path (or list of multiple paths) that lead to the dataset passed to the function call. In the custom functions, this will pass as the first (and successive, in case of multiple paths) positional argument to said custom function. Note that all other arguments passed will be keyword arguments.\n",
    "\n",
    "The optional arguments are:\n",
    "\n",
    "1. `output_names`: This is the name of the actual dataset produced in the `.hdf5` file. If this is left unset, it will inherit the name of the first source file. Alternatively, if output_names is a list where `len(output_names) > 1`, the Python will know to prepare additional output files. This is required if additional outputs are desired.\n",
    "2. `folder_names`: The name of the folder that the outputs lie in. This is only a single string; it cannot be a list, like `output_names` can be.\n",
    "3. `use_attrs`: Attributes that will be given to the function. This can either be a string, or a list of strings. `m_apply` will look through each source to find an attribute that bears the same name as the string declared. This is done in order; if multiple sources are used, and each have the same attribute, the attribute of the first list will be used. When the custom function is then called, an additional kwarg is submitted, bearing the name of the string, preceeded by `'source_'`. So, if a file has attributes `'base_attack':134` and `'move':'outrage'`, then declaring `use_attrs = ['base_attack', 'move']` would pass two additional arguments to the custom function: `source_base_attack = 134` and `source_move = 'outrage'`. If the attribute is not present, then the extra kwarg will not be passed; if the kwarg is needed, be sure to note it in the custom function.\n",
    "4. `prop_attrs`: These are attributes that are simply propagated from the source into the destination. As with `use_attrs`, these will be searched in order, and ignored if not found.\n",
    "5. `increment_proc`: This should not be directly called, and only exists to interface with `l_apply`. By default, the process number increases each time (from 001, to 002, to 003, ...). By default, if `l_apply` were to operate 5 times, then 5 distinct processes would be made. `l_apply` thus sets this to `False` on subsequent operations to ensure the folder is kept the same.\n",
    "\n",
    "Finally, optional kwargs can be provided\n",
    "\n",
    "1. `**kwargs`: These kwargs are passed into the custom function. `m_apply` also has an additional use, though; every kwarg passed in is automatically written as an attribute into the dataset. Thus, setting a kwarg `opponent_type = 'Steel'` will cause an attribute called `opponent_type` to be written with a value of `'Steel'`. Following the above example, as `use_attrs` automatically passes its values as kwargs to the custom function, these are also made to be attributes. Thus, given the arguments defined with `prop_attrs`, both `source_base_attack = 134` and `source_move = 'outrage'` will also be written as attributes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l_apply and path_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_apply(filename, function, all_input_criteria, output_names = None, folder_names = None, \n",
    "            prop_attrs = None, repeat = None, **kwargs):\n",
    "    all_in_path_list = path_search(filename, all_input_criteria, repeat)\n",
    "    all_in_path_list = list(map(list, zip(*all_in_path_list)))\n",
    "    increment_proc = True\n",
    "    start_time = time.time()\n",
    "    for path_num in range(len(all_in_path_list)):\n",
    "        m_apply(filename, function, all_in_path_list[path_num], output_names = output_names,\n",
    "                folder_names = folder_names, increment_proc = increment_proc,\n",
    "                prop_attrs = prop_attrs, **kwargs)\n",
    "        progress_report(path_num+1, len(all_in_path_list), start_time, function.__name__,\n",
    "                        all_in_path_list[path_num])\n",
    "        increment_proc = False    \n",
    "        \n",
    "def path_search(filename, all_input_criteria, repeat = None):\n",
    "    if type(all_input_criteria) != list:\n",
    "        all_input_criteria = [all_input_criteria]\n",
    "    if type(all_input_criteria[0]) != list:\n",
    "        all_input_criteria = [all_input_criteria]\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        all_path_list = find_paths_of_all_subgroups(f, 'datasets')\n",
    "        all_path_list.extend(find_paths_of_all_subgroups(f, 'process'))\n",
    "        \n",
    "        all_in_path_list = []\n",
    "        list_lengths = []\n",
    "        for each_data_type in all_input_criteria:\n",
    "            in_path_list = []\n",
    "            for each_criteria in each_data_type:\n",
    "                for path in all_path_list:\n",
    "                    if fnmatch.fnmatch(path, each_criteria):\n",
    "                        in_path_list.append(path)\n",
    "            all_in_path_list.append(in_path_list)\n",
    "            list_lengths.append(len(in_path_list))\n",
    "        if len(list_lengths) == 1:\n",
    "            if list_lengths[0] == 0:\n",
    "                print('No Input Datafiles found!')\n",
    "        else:\n",
    "            if len(set(list_lengths)) != 1:\n",
    "                if repeat is None:\n",
    "                    print('Input lengths not equal, and repeat not set! Extra files will be omitted.')\n",
    "                else:\n",
    "                    largest_list_length = np.max(list_lengths)\n",
    "                    list_multiples = []\n",
    "                    for length in list_lengths:\n",
    "                        if largest_list_length%length != 0:\n",
    "                            print('At least one path list length is not a factor of the largest path'\\\n",
    "                                  'list length. Extra files will be omitted.')\n",
    "                        list_multiples.append(largest_list_length//length)\n",
    "                    if (repeat == 'block') or (repeat == 'b'):\n",
    "                        for list_num in range(len(list_multiples)):\n",
    "                            all_in_path_list[list_num] = np.repeat(all_in_path_list[list_num],\n",
    "                                                                   list_multiples[list_num])\n",
    "                    if (repeat == 'alt') or (repeat == 'a'):\n",
    "                        for list_num in range(len(list_multiples)):\n",
    "                            old_path_list = all_in_path_list[list_num]\n",
    "                            new_path_list = []\n",
    "                            for repeat_iter in range(list_multiples[list_num]):\n",
    "                                new_path_list.extend(old_path_list)\n",
    "                            all_in_path_list[list_num] = new_path_list\n",
    "    return all_in_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`l_apply` effectively calls `m_apply` several times. Thus, the arguments of `m_apply` are effectively identical to `l_apply`. The one difference is that `output_names`, which used to explicitly call a folder, has now been replaced with two new arguments:\n",
    "\n",
    "1. `all_input_criteria`: The use of this argument has generally been described in the Intermediate tutorial. Effectively, this is a two dimensional list; if less than two dimensions are submitted, the extra dimensions are added along the ends. The inner list is a list of search conditions that can be used with wildcards. Multiple conditions are used, so searching for `[['*Phase*', '*Amplitude*']]` would search for both conditions and send each to `m_apply` only after the previous dataset is done with.\n",
    "\n",
    "The outer list is only used if the custom function calls for two or more datasets; in that case, a second condition could be applied. Searching for `[['*Phase*'], ['*Amplitude*']]` would thus send two arrays to the custom function on each operation; a phase, and an amplitude.\n",
    "\n",
    "2. `repeat`: This increases the ease of using `l_apply` when multiple datasets are needed, and some of these datasets would need to be used multiple times. This can occur if, for example, `n` parameters need to be applied to `8n` arrays. In this case, you would want `n` to repeat 8 times. There are two options for how to use `repeat`. The first, `alt`, or `a`, repeats the shorter path list in its entirety; so a path list `ABC` would repeat to be `ABCABCABC...`. The other option, `block`, or `b`, repeats  each individual component, causing 'phase separation' in a classical sense. A path list `ABC` would thus become `AAA...BBB...CCC...`, where `A`, `B` and `C` are all of equal length.\n",
    "\n",
    "The function `path_search` can also be called separate from `l_apply`. This can allow for more complicated custom functions to be utilise the search conditions and wildcares, even if they cannot necessarily use `l_apply` or `m_apply` in their entirety.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hdf5_dict and Attribute Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf5_dict(data, **kwargs):\n",
    "    data_dict = {\n",
    "        'hdf5_dict':True,\n",
    "        'data':data\n",
    "    }\n",
    "    data_dict.update(kwargs)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to write attributes. As described above, there are five-ish ways that these attributes are written:\n",
    "\n",
    "* Basic attributes are written by `write_generic_attributes`, which is called by `m_apply`\n",
    "* kwargs passed into `m_apply` are written as attributes\n",
    "* Source attributes passed into `m_apply` via `use_attrs`, which become kwargs, are also written\n",
    "* Attributes can be copied and propagated from sources by using the argument `prop_attrs` in `m_apply`.\n",
    "\n",
    "The final method - using `hdf5_dict` - needs to be used in custom functions. This allows intermediate or side results to be saved as attributes. To use `hdf5_dict`, it must be called within the function. Consider the custom function m_sum below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_sum(*args):\n",
    "    total = 0\n",
    "    for arg in args:\n",
    "        total = total+arg\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, if called by `m_apply`, would add entries of a list together. Say that an attribute that reported the amount of entries added together was desired. This value, `input_count`, could be defined by `len(args)`. Simply returning `input_count` would cause Multiscale to believe it to be another dataset, and thus it needs to be combined with the actual dataset as a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_sum(*args):\n",
    "    total = 0\n",
    "    for arg in args:\n",
    "        total = total+arg\n",
    "        \n",
    "    #return total\n",
    "    \n",
    "    result = hdf5_dict(total, input_count=input_count)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hdf5_dict` then creates a `dict`, which in this case has three keys: `'hdf5_dict'`, which marks it as being made by the function of the same name; `'data'`, which contains the dataset; and `'input_count'`, which is the attribute, whose name is defined in the `hdf5_dict` function call. If additional attributes were desired, these would be additional keys in the dict.\n",
    "\n",
    "After this, `create_dataset_from_dict` can then called by `m_apply` to write these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_dict (dataset, name, dict_data):\n",
    "    dataset = dataset.create_dataset(name, data = dict_data['data'])\n",
    "    for key, value in dict_data.items():\n",
    "        if (key != 'hdf5_dict') and (key != 'data'):\n",
    "            dataset.attrs[key] = value\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
